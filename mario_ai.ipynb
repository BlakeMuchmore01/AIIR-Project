{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIIR Project - AI Mario\n",
    "This jupyter notebook contains the application of nueral network and reinforcement learning algorithms learnt from the tutorials to simulate Mario completing a variety of levels in a pybullet gym environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Mario Environment\n",
    "We use a Super Mario Bros environment (https://pypi.org/project/gym-super-mario-bros/) with a continuous state space and discrete action space. The goal of this activity is to complete Mario levels as fast as possible. Episodes end when Mario reaches the end of the level, if Mario dies, or if a certain time as elapsed.\n",
    "\n",
    "### Action Space\n",
    "- 0: No Movement\n",
    "- 1: Move Right\n",
    "- 2: Move Right + Jump\n",
    "- 3: Move Right + Speed Up\n",
    "- 4: Move Right + Jump + Speed Up\n",
    "- 5: Jump\n",
    "- 6: Move Left\n",
    "- 7: Move Left + Jump\n",
    "- 8: Move Left + Speed Up\n",
    "- 9: Move Left + Jump + Speed Up\n",
    "- 10: Down\n",
    "- 11: Up\n",
    "\n",
    "### Observation Space\n",
    "The info dictionary returned by step contains the following:\n",
    "| Key | Unit | Description |\n",
    "| --- | ---- | ----------- |\n",
    "| coins | int | Number of collected coins |\n",
    "| flag_get | bool | True if Mario reached a flag |\n",
    "| life | int | Number of lives left |\n",
    "| score | int | Cumulative in-game score |\n",
    "| stage | int | Current stage |\n",
    "| status | str | Mario's status/power |\n",
    "| time | int | Time left on the clock |\n",
    "| world | int | Current world |\n",
    "| x_pos | int | Mario's x position in the stage |\n",
    "| y_pos | int | Mario's y position in the stage |\n",
    "\n",
    "### Rewards\n",
    "| Feature | Description | Value when Positive | Value when Negative | Value when Equal |\n",
    "|---------|-------------|---------------------|---------------------|------------------|\n",
    "| Difference in agent x values between states | Controls agent's movement | Moving right | Moving left | Not moving |\n",
    "| Time difference in the game clock between frames | Prevents agent from staying still | - | Clock ticks | Clock doesn't tick |\n",
    "| Death Penalty | Discourages agent from death | - | Agent dead | Agent alive |\n",
    "| - | - | - | - | - |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation Guide\n",
    "For installing the Super Mario Bros gym environment package, use the following command using a python 3.8 kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gym-super-mario-bros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessary packages and following helper function to display video runs within jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYVIRTUALDISPLAY_DISPLAYFD'] = '0' \n",
    "\n",
    "import gym\n",
    "import pybullet as p\n",
    "import matplotlib.pyplot as plt\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython.display import HTML\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "display = Display(visible=0, size=(400, 300))\n",
    "display.start()\n",
    "\n",
    "def display_video(frames, framerate=30):\n",
    "  \"\"\"Generates video from `frames`.\n",
    "\n",
    "  Args:\n",
    "    frames (ndarray): Array of shape (n_frames, height, width, 3).\n",
    "    framerate (int): Frame rate in units of Hz.\n",
    "\n",
    "  Returns:\n",
    "    Display object.\n",
    "  \"\"\"\n",
    "  height, width, _ = frames[0].shape\n",
    "  dpi = 70\n",
    "  orig_backend = matplotlib.get_backend()\n",
    "  matplotlib.use('Agg')  # Switch to headless 'Agg' to inhibit figure rendering.\n",
    "  fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n",
    "  matplotlib.use(orig_backend)  # Switch back to the original backend.\n",
    "  ax.set_axis_off()\n",
    "  ax.set_aspect('equal')\n",
    "  ax.set_position([0, 0, 1, 1])\n",
    "  im = ax.imshow(frames[0])\n",
    "  def update(frame):\n",
    "    im.set_data(frame)\n",
    "    return [im]\n",
    "  interval = 1000/framerate\n",
    "  anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
    "                                  interval=interval, blit=True, repeat=False)\n",
    "  return HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EPISODES = 2500                  # Number of episodes to run for training\n",
    "LEARNING_RATE = 0.00025          # Learning rate for optimizing neural network weights\n",
    "MEM_SIZE = 50000                 # Maximum size of replay memory\n",
    "REPLAY_START_SIZE = 10000        # Amount of samples to fill replay memory before training\n",
    "BATCH_SIZE = 32                  # Number of samples to draw from replay memory for training\n",
    "GAMMA = 0.99                     # Discount factor for future rewards\n",
    "EPSILON_START = 0.1              # Starting exploration rate\n",
    "EPSILON_END = 0.0001             # Final exploration rate\n",
    "EPSILON_DECAY = 4 * MEM_SIZE     # Decay rate for exploration rate\n",
    "MEM_RETAIN = 0.1                 # Percentage of memory to retain on each episode\n",
    "NETWORK_UPDATE_ITERS = 5000      # Number of steps to update target network\n",
    "\n",
    "FC1_DIMS = 128                   # Number of neurons in our MLP's first hidden layer\n",
    "FC2_DIMS = 128                   # Number of neurons in our MLP's second hidden layer\n",
    "\n",
    "# Metrics for displaying training status\n",
    "best_reward = 0\n",
    "average_reward = 0\n",
    "episode_history = []\n",
    "episode_reward_history = []\n",
    "np.bool = np.bool_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "Below is the class definition for a neural network used to approximate Q-values for the use within a reinforcement learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Neural network class used to approximate Q-values within the Mario environment\n",
    "class Network(torch.nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super(Network, self).__init__()  # Inheriting from torch.nn.Module\n",
    "\n",
    "        self.input_shape = env.observation_space.shape  # Getting shape of observation space\n",
    "        self.action_space = env.action_space.n          # Getting number of actions in action space\n",
    "\n",
    "        # Defining the layers of the neural network\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(*self.input_shape, FC1_DIMS),   # \n",
    "            torch.nn.ReLU(),                                # \n",
    "            torch.nn.Linear(FC1_DIMS, FC2_DIMS),            #\n",
    "            torch.nn.ReLU(),                                #\n",
    "            torch.nn.Linear(FC2_DIMS, self.action_space)    #\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=LEARNING_RATE)  # Optimizer\n",
    "        self.loss = nn.MSELoss()  # Loss Function\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)  # Forward pass through the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the AI\n",
    "The following code uses the above Neural Network and Reinforcement Learning framework to train the AI to play Super Mario Bros on a variety of its levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import COMPLEX_MOVEMENT\n",
    "import gym\n",
    "\n",
    "env = gym_super_mario_bros.make('SuperMarioBrosRandomStages-v0', apply_api_compatibility=True, render_mode=\"rgb_array\")\n",
    "env = JoypadSpace(env, COMPLEX_MOVEMENT)\n",
    "\n",
    "state = env.reset()\n",
    "frames = []\n",
    "frames.append(env.render())\n",
    "\n",
    "for step in range(EPISODES/EPISODES * 200):\n",
    "    action = env.action_space.sample()\n",
    "    state_, reward, done, truncated, info = env.step(action)\n",
    "    frames.append(np.copy(env.render()))\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "display_video(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the AI\n",
    "The following code tests the AI policy generated during training in random Super Mario Bros levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym_super_mario_bros.make('SuperMarioBrosRandomStages-v0', apply_api_compatibility=True, render_mode=\"rgb_array\")\n",
    "env = JoypadSpace(env, COMPLEX_MOVEMENT)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
