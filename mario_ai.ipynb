{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIIR Project - AI Mario\n",
    "This jupyter notebook contains the application of nueral network and reinforcement learning algorithms learnt from the tutorials to simulate Mario completing a variety of levels in a pybullet gym environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Mario Environment\n",
    "We use a Super Mario Bros environment (https://pypi.org/project/gym-super-mario-bros/) with a continuous state space and discrete action space. The goal of this activity is to complete Mario levels as fast as possible. Episodes end when Mario reaches the end of the level, if Mario dies, or if a certain time as elapsed.\n",
    "\n",
    "### Action Space\n",
    "- 0: No Movement\n",
    "- 1: Move Right\n",
    "- 2: Move Right + Jump\n",
    "- 3: Move Right + Speed Up\n",
    "- 4: Move Right + Jump + Speed Up\n",
    "- 5: Jump\n",
    "- 6: Move Left\n",
    "- 7: Move Left + Jump\n",
    "- 8: Move Left + Speed Up\n",
    "- 9: Move Left + Jump + Speed Up\n",
    "- 10: Down\n",
    "- 11: Up\n",
    "\n",
    "### Observation Space\n",
    "The info dictionary returned by step contains the following:\n",
    "| Key | Unit | Description |\n",
    "| --- | ---- | ----------- |\n",
    "| coins | int | Number of collected coins |\n",
    "| flag_get | bool | True if Mario reached a flag |\n",
    "| life | int | Number of lives left |\n",
    "| score | int | Cumulative in-game score |\n",
    "| stage | int | Current stage |\n",
    "| status | str | Mario's status/power |\n",
    "| time | int | Time left on the clock |\n",
    "| world | int | Current world |\n",
    "| x_pos | int | Mario's x position in the stage |\n",
    "| y_pos | int | Mario's y position in the stage |\n",
    "\n",
    "### Rewards\n",
    "| Feature | Description | Value when Positive | Value when Negative | Value when Equal |\n",
    "|---------|-------------|---------------------|---------------------|------------------|\n",
    "| Difference in agent x values between states | Controls agent's movement | Moving right | Moving left | Not moving |\n",
    "| Time difference in the game clock between frames | Prevents agent from staying still | - | Clock ticks | Clock doesn't tick |\n",
    "| Death Penalty | Discourages agent from death | - | Agent dead | Agent alive |\n",
    "| Coins | Encourages agent to get coins | Coin collected | - | No coin collected |\n",
    "| Score | Encourages agent to get higher score | Score Value | Score Value | Score Value |\n",
    "| Flag | Encourages agent to reach middle & end flag | Flag collected | - | Flag not collected |\n",
    "| Powerup | Encourages agent to get powerups | Powerup collected | - | Powerup not collected |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation Guide\n",
    "For installing the Super Mario Bros gym environment package, use the following command using a python 3.8 kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gym-super-mario-bros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessary packages and following helper function to display video runs within jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYVIRTUALDISPLAY_DISPLAYFD'] = '0' \n",
    "\n",
    "import gym\n",
    "import pybullet as p\n",
    "import matplotlib.pyplot as plt\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython.display import HTML\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import math\n",
    "\n",
    "display = Display(visible=0, size=(400, 300))\n",
    "display.start()\n",
    "\n",
    "def display_video(frames, framerate=30):\n",
    "  \"\"\"Generates video from `frames`.\n",
    "\n",
    "  Args:\n",
    "    frames (ndarray): Array of shape (n_frames, height, width, 3).\n",
    "    framerate (int): Frame rate in units of Hz.\n",
    "\n",
    "  Returns:\n",
    "    Display object.\n",
    "  \"\"\"\n",
    "  height, width, _ = frames[0].shape\n",
    "  dpi = 70\n",
    "  orig_backend = matplotlib.get_backend()\n",
    "  matplotlib.use('Agg')  # Switch to headless 'Agg' to inhibit figure rendering.\n",
    "  fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n",
    "  matplotlib.use(orig_backend)  # Switch back to the original backend.\n",
    "  ax.set_axis_off()\n",
    "  ax.set_aspect('equal')\n",
    "  ax.set_position([0, 0, 1, 1])\n",
    "  im = ax.imshow(frames[0])\n",
    "  def update(frame):\n",
    "    im.set_data(frame)\n",
    "    return [im]\n",
    "  interval = 1000/framerate\n",
    "  anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
    "                                  interval=interval, blit=True, repeat=False)\n",
    "  return HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EPISODES = 2500                  # Number of episodes to run for training\n",
    "LEARNING_RATE = 0.00025          # Learning rate for optimizing neural network weights\n",
    "MEM_SIZE = 50000                 # Maximum size of replay memory\n",
    "REPLAY_START_SIZE = 10000        # Amount of samples to fill replay memory before training\n",
    "BATCH_SIZE = 32                  # Number of samples to draw from replay memory for training\n",
    "GAMMA = 0.99                     # Discount factor for future rewards\n",
    "EPSILON_START = 0.1              # Starting exploration rate\n",
    "EPSILON_END = 0.0001             # Final exploration rate\n",
    "EPSILON_DECAY = 4 * MEM_SIZE     # Decay rate for exploration rate\n",
    "MEM_RETAIN = 0.1                 # Percentage of memory to retain on each episode\n",
    "NETWORK_UPDATE_ITERS = 5000      # Number of steps to update target network             \n",
    "\n",
    "FC1_DIMS = 512                   # Number of neurons in first fully connected layer\n",
    "FC2_DIMS = 512                   # Number of neurons in second fully connected layer\n",
    "\n",
    "# Metrics for displaying training status\n",
    "best_reward = 0\n",
    "average_reward = 0\n",
    "episode_history = []\n",
    "episode_reward_history = []\n",
    "np.bool = np.bool_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "Below is the class definition for a neural network used to approximate Q-values for the use within a reinforcement learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Neural network class to approximate Q-values\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, env, freeze=False):\n",
    "        super().__init__()  # Inheriting from torch.nn.Module\n",
    "\n",
    "        self.input_shape = env.observation_space.shape  # Getting shape of observation space\n",
    "        self.action_space = env.action_space.n          # Getting number of actions in action space\n",
    "\n",
    "        # Defining convolutional layers\n",
    "        # Convolutional Neural Network (CNN) used for image inputs\n",
    "        self.conv_layers = torch.nn.Sequential(\n",
    "            nn.Conv2d(self.input_shape[0], 32, kernel_size=1, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=1, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=1, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Getting the output size of the convolutional layers\n",
    "        conv_out_size = self._get_conv_out(self.input_shape)\n",
    "\n",
    "        # Defining the linear layers\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            self.conv_layers,\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(conv_out_size, FC1_DIMS),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(FC1_DIMS, FC2_DIMS),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(FC2_DIMS, self.action_space)\n",
    "        )\n",
    "\n",
    "        if freeze:\n",
    "            self._freeze()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=LEARNING_RATE)  # Optimizer\n",
    "        self.loss = nn.MSELoss()  # Loss Function\n",
    "\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Device to run the network on\n",
    "        self.to(self.device)  # Moving the network to the device\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)  # Forward pass through the network\n",
    "    \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv_layers(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    # Function to freeze NN learning\n",
    "    def _freeze(self):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Buffer class for storing and retrieving sampled experiences\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, env):\n",
    "        # Initialising memory count and creating arrays to store experiences\n",
    "        self.mem_count = 0\n",
    "        self.states = np.zeros((MEM_SIZE, *env.observation_space.shape),dtype=np.float32)\n",
    "        self.actions = np.zeros(MEM_SIZE, dtype=np.int64)\n",
    "        self.rewards = np.zeros(MEM_SIZE, dtype=np.float32)\n",
    "        self.states_ = np.zeros((MEM_SIZE, *env.observation_space.shape),dtype=np.float32)\n",
    "        self.dones = np.zeros(MEM_SIZE, dtype=np.bool)\n",
    "\n",
    "    def add(self, state, action, reward, state_, done):\n",
    "        # If memory count is at max size, overwrite previous values\n",
    "        if self.mem_count < MEM_SIZE:\n",
    "            mem_index = self.mem_count\n",
    "        else:\n",
    "            # Avoiding catastrophic forgetting - retrain initial 10% of the replay buffer\n",
    "            mem_index = int(self.mem_count % ((1-MEM_RETAIN) * MEM_SIZE) + (MEM_RETAIN * MEM_SIZE))\n",
    "\n",
    "        self.states[mem_index]  = state     # Storing the state\n",
    "        self.actions[mem_index] = action    # Storing the action\n",
    "        self.rewards[mem_index] = reward    # Storing the reward\n",
    "        self.states_[mem_index] = state_    # Storing the next state\n",
    "        self.dones[mem_index] =  1 - done   # Storing the done flag\n",
    "        self.mem_count += 1                 # Incrementing memory count\n",
    "    \n",
    "    def sample(self):\n",
    "        # Randomly sample a batch of experiences\n",
    "        MEM_MAX = min(self.mem_count, MEM_SIZE)\n",
    "        batch_indices = np.random.choice(MEM_MAX, BATCH_SIZE, replace=True)\n",
    "\n",
    "        states  = self.states[batch_indices]    # Getting the states\n",
    "        actions = self.actions[batch_indices]   # Getting the actions\n",
    "        rewards = self.rewards[batch_indices]   # Getting the rewards\n",
    "        states_ = self.states_[batch_indices]   # Getting the next states\n",
    "        dones   = self.dones[batch_indices]     # Getting the done flags\n",
    "\n",
    "        # Returning the random sampled experiences\n",
    "        return states, actions, rewards, states_, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.memory = ReplayBuffer(env)     # Creating a replay buffer\n",
    "        self.policy_network = NeuralNetwork(env)  # Q\n",
    "        self.target_network = NeuralNetwork(env)  # \\hat{Q}\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())  # Initially set weights of Q to \\hat{Q}\n",
    "        self.learn_count = 0    # keep track of the number of iterations we have learnt for\n",
    "\n",
    "    # Epsilon-greedy policy\n",
    "    def choose_action(self, observation):\n",
    "        # Only start decaying epsilon once we start learning (once replay memory has REPLAY_START_SIZE samples)\n",
    "        if self.memory.mem_count > REPLAY_START_SIZE:\n",
    "            eps_threshold = EPSILON_END + (EPSILON_START - EPSILON_END) * \\\n",
    "                math.exp(-1. * self.learn_count / EPSILON_DECAY)\n",
    "        else:\n",
    "            eps_threshold = 1.0\n",
    "\n",
    "        # If we rolled a value lower than the epsilon sample a random action\n",
    "        if random.random() < eps_threshold:\n",
    "            return np.random.choice(np.array(range(12)), p=[0.05, 0.1, 0.1, 0.1, 0.1, 0.05, 0.1, 0.1, 0.1, 0.1, 0.05, 0.05])  # Random action with set priors\n",
    "        \n",
    "        # Otherwise policy network (Q) chooses action with highest estimated Q value so far\n",
    "        state = torch.tensor(observation).float().detach()\n",
    "        state = state.unsqueeze(0)\n",
    "        self.policy_network.eval()\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_network(state)  # Get Q-values from policy network\n",
    "\n",
    "        return torch.argmax(q_values).item()\n",
    "    \n",
    "    # Main training loop\n",
    "    def learn(self):\n",
    "        states, actions, rewards, states_, dones = self.memory.sample()  # Sample a batch of random experiences\n",
    "        states = torch.tensor(states, dtype=torch.float32)      # Convert states to tensor\n",
    "        actions = torch.tensor(actions, dtype=torch.long)       # Convert actions to tensor\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)    # Convert rewards to tensor\n",
    "        states_ = torch.tensor(states_, dtype=torch.float32)    # Convert next states to tensor\n",
    "        dones = torch.tensor(dones, dtype=torch.bool)           # Convert done flags to tensor\n",
    "        batch_indices = np.arange(BATCH_SIZE, dtype=np.int64)   # Create an array of batch indices\n",
    "\n",
    "        self.policy_network.train(True)\n",
    "        q_values = self.policy_network(states)  # Get Q-value estimates from policy network\n",
    "        q_values = q_values[batch_indices, actions]  # Get Q-values for the sampled actions\n",
    "\n",
    "        self.target_network.eval()\n",
    "        with torch.no_grad():\n",
    "            q_values_next = self.target_network(states_)  # Get Q-values of states_ from target nework (Q_hat)\n",
    "\n",
    "        q_values_next_max = torch.max(q_values_next, dim=1)[0]  # Max Q-values for next state\n",
    "        q_target = rewards + GAMMA * q_values_next_max * dones  # Calculate target Q-values\n",
    "\n",
    "        loss = self.policy_network.loss(q_values, q_target)  # Calculate loss from target Q-values and predicted Q-values\n",
    "\n",
    "        # Compute gradients and update Q weights\n",
    "        self.policy_network.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.policy_network.optimizer.step()  # Update Q weights\n",
    "        self.learn_count += 1  # Increment learn count\n",
    "\n",
    "        # Set target network weights to policy networks wights every C steps\n",
    "        if self.learn_count % NETWORK_UPDATE_ITERS == NETWORK_UPDATE_ITERS - 1:\n",
    "            print(\"Updating target network\")\n",
    "            self.update_target_network()\n",
    "        \n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "\n",
    "    def returning_epsilon(self):\n",
    "        return self.exploration_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the AI\n",
    "The following code uses the above Neural Network and Reinforcement Learning framework to train the AI to play Super Mario Bros on a variety of its levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply additional rewards that aren't in the environment already\n",
    "def reward_shaping(prev_info, info):\n",
    "    shapedReward = 0  # Container to store the additional reward\n",
    "    reward_values = {  # Container to store keys for rewards\n",
    "        'coins': 1,\n",
    "        'score': lambda previous, current: current - previous,\n",
    "        'flag_get': 50,\n",
    "        'powerup': lambda previous, current: 10 if current > previous else 0\n",
    "    }\n",
    "\n",
    "    # Applying the reward values to the shaped reward\n",
    "    for key, reward in reward_values.items():\n",
    "        prev_value = prev_info.get(key, 0)  # Getting the previous info values for keys\n",
    "        curr_value = info.get(key, 0)       # Getting the current info values for keys\n",
    "\n",
    "        # If the reward is a function, apply the function to the previous and current values\n",
    "        if callable(reward):\n",
    "            shapedReward += reward(prev_value, curr_value)\n",
    "\n",
    "        # Otherwise, apply the reward value to the shaped reward\n",
    "        elif curr_value > prev_value:\n",
    "            shapedReward += reward\n",
    "\n",
    "    return shapedReward  # Return the shaped reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device: NVIDIA RTX A2000 Laptop GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/blakemuchmorewsl/git/AIIR-Project/.venv/lib/python3.8/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBrosRandomStages-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/blakemuchmorewsl/git/AIIR-Project/.venv/lib/python3.8/site-packages/gym/envs/registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  logger.warn(\n",
      "/home/blakemuchmorewsl/git/AIIR-Project/.venv/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 36\u001b[0m\n\u001b[1;32m     30\u001b[0m state_, reward, done, _, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)        \u001b[38;5;66;03m# Taking a step in the environment\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m## Adding additional reward systems ##\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#if prev_info is not None:\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#    reward += reward_shaping(prev_info, info)\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# Adding experience to replay buffer\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Learning from experiences in replay buffer once start size is reached\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mmem_count \u001b[38;5;241m>\u001b[39m REPLAY_START_SIZE:\n",
      "Cell \u001b[0;32mIn[35], line 20\u001b[0m, in \u001b[0;36mReplayBuffer.add\u001b[0;34m(self, state, action, reward, state_, done)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Avoiding catastrophic forgetting - retrain initial 10% of the replay buffer\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     mem_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem_count \u001b[38;5;241m%\u001b[39m ((\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mMEM_RETAIN) \u001b[38;5;241m*\u001b[39m MEM_SIZE) \u001b[38;5;241m+\u001b[39m (MEM_RETAIN \u001b[38;5;241m*\u001b[39m MEM_SIZE))\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmem_index\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;241m=\u001b[39m state     \u001b[38;5;66;03m# Storing the state\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions[mem_index] \u001b[38;5;241m=\u001b[39m action    \u001b[38;5;66;03m# Storing the action\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards[mem_index] \u001b[38;5;241m=\u001b[39m reward    \u001b[38;5;66;03m# Storing the reward\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import COMPLEX_MOVEMENT\n",
    "import gym\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using CUDA device:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available\")\n",
    "\n",
    "# Loading the Super Mario Bros envrionment and setting movement mode\n",
    "env = gym_super_mario_bros.make('SuperMarioBrosRandomStages-v0', apply_api_compatibility=True, render_mode=\"rgb_array\")\n",
    "env = JoypadSpace(env, COMPLEX_MOVEMENT)\n",
    "\n",
    "episode_batch_score = 0     # Initialize episode score\n",
    "episode_reward = 0          # Initialize episode reward\n",
    "prev_info = None            # Initialize previous info container\n",
    "agent = Agent(env)          # Create RL agent\n",
    "plt.clf()                   # Clear previous plot\n",
    "\n",
    "env.reset()  # Reseting environment\n",
    "state_, reward, done, trunc, info = env.step(action=0)  # Taking a step in the environment\n",
    "\n",
    "for i in range(EPISODES):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        action = agent.choose_action(state)                     # Choosing action\n",
    "        state_, reward, done, _, info = env.step(action)        # Taking a step in the environment\n",
    "\n",
    "        ## Adding additional reward systems ##\n",
    "        #if prev_info is not None:\n",
    "        #    reward += reward_shaping(prev_info, info)\n",
    "\n",
    "        agent.memory.add(state, action, reward, state_, done)   # Adding experience to replay buffer\n",
    "        # Learning from experiences in replay buffer once start size is reached\n",
    "        if agent.memory.mem_count > REPLAY_START_SIZE:\n",
    "            agent.learn()\n",
    "\n",
    "        state = state_                  # Updating state\n",
    "        prev_info = info                # Updating previous info\n",
    "        episode_batch_score += reward   # Updating episode score\n",
    "        episode_reward += reward        # Updating episode reward\n",
    "\n",
    "    episode_history.append(i)                       # Appending episode number to history\n",
    "    episode_reward_history.append(episode_reward)   # Appending episode reward to history\n",
    "    episode_reward = 0                              # Resetting episode reward\n",
    "\n",
    "    # Saving the model every 100 episodes\n",
    "    if i % 100 == 0 and agent.memory.mem_count > REPLAY_START_SIZE:\n",
    "        save_path = os.path.join(os.getcwd(), \"policy_network.pkl\")\n",
    "        torch.save(agent.policy_network.state_dict(), save_path)\n",
    "        print(\"average total reward per episode batch since episode \", i, \": \", episode_batch_score/ float(100))\n",
    "        episode_batch_score = 0\n",
    "    elif agent.memory.mem_count < REPLAY_START_SIZE:\n",
    "        print(\"waiting for buffer to fill...\")\n",
    "        episode_batch_score = 0\n",
    "\n",
    "plt.plot(episode_history, episode_reward_history)\n",
    "plt.show()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the AI\n",
    "The following code tests the AI policy generated during training in random Super Mario Bros levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym_super_mario_bros.make('SuperMarioBrosRandomStages-v0', apply_api_compatibility=True, render_mode=\"rgb_array\")\n",
    "env = JoypadSpace(env, COMPLEX_MOVEMENT)\n",
    "\n",
    "agent = Agent(env)\n",
    "agent.policy_network.load_state_dict(torch.load(\"policy_network.pkl\"))\n",
    "state = env.reset()\n",
    "frames = []\n",
    "frames.append(env.render())\n",
    "agent.policy_network.eval()\n",
    "\n",
    "while True:\n",
    "    with torch.no_grad():\n",
    "        q_values = agent.policy_network(torch.tensort(state, dtype=torch.float32))\n",
    "\n",
    "    action = torch.argmax(q_values).item()\n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "    frames.append(np.copy(env.render()))\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "display_video(frames)                                  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
